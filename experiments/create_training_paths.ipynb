{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7846324d",
   "metadata": {},
   "source": [
    "First we create a balanced set of images for adapting the weights of naphash \n",
    "In order 16384 frames from each of these datasets:\n",
    "* CelebA https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html\n",
    "* COCO https://cocodataset.org/#home\n",
    "* Alternative ImageNet datasets  \"ImageNetV2\"( https://github.com/modestyachts/ImageNetV2 ) and ImageNet-Sketch (https://github.com/HaohanWang/ImageNet-Sketch)\n",
    "* Fashionpedia Dataset https://fashionpedia.github.io/home/Fashionpedia_download.html\n",
    "* iNaturalist dataset 2019 https://www.kaggle.com/c/inaturalist-2019-fgvc6\n",
    "* Places365-Standard http://places2.csail.mit.edu/download.html\n",
    "* ImageNet fall11 Release https://www.image-net.org/\n",
    "\n",
    "This will also load the images, calculate the dct, and store it on disk for faster weight adaption training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6701658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#There are three hyper-parameters for weight adaption session:\n",
    "dct_dim = 32\n",
    "min_img_dims = 128\n",
    "trg_num_samples = 16384 #largest power of two which can fit in the individual dataset sizes (Places365 is 36500 but with the potential amount of large/small images we stay on the cautious side)\n",
    "\n",
    "#Experimental switches:\n",
    "use_pil_rz = False #uses PIL LANCZOS for downsampling instead of opencv INTER_AREA  \n",
    "num_threads = 8 #use multi-core procedures with this many cores where possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cf5713",
   "metadata": {},
   "source": [
    "Each dataset as a different number of samples and some contain a different degree of images too small for use (128x128 is used as a minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e982ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import glob\n",
    "seekall = '/workspace/data/data/imagenet_fall11/fall11_whole.seekhelper.txt.gz'\n",
    "def load_from_gz(filepath, tar_root_folder=\"\", as_list_style=True):\n",
    "    if as_list_style:\n",
    "        info = []\n",
    "    else:\n",
    "        info = {}\n",
    "    with gzip.open(filepath, 'rb') as f_in:\n",
    "        for line in f_in:\n",
    "            if as_list_style:\n",
    "                info.append(tar_root_folder+line.decode('ascii'))\n",
    "            else:\n",
    "                l_sp = line.decode('ascii').split(':')\n",
    "                info[l_sp[0]] = [int(l_sp[1]),int(l_sp[2])]\n",
    "    return info\n",
    "\n",
    "def load_from_seekhelper(dir0):\n",
    "    if not dir0[-1] == '/':\n",
    "        dir0+='/'\n",
    "    return [dir0+p for p in load_from_gz(dir0+'.seekhelper.txt.gz')]\n",
    "imgnet_alt_paths = load_from_seekhelper('/workspace/data/data/imagenet_alt/')\n",
    "fashionpedia_paths = load_from_seekhelper('/workspace/data/data/fashionpedia/')\n",
    "inat2019_paths = load_from_seekhelper('/workspace/data/inat2019/')\n",
    "places365_paths = load_from_seekhelper('/workspace/data/places365/')\n",
    "celeba_paths = load_from_seekhelper('/workspace/data/celeba/jpg256')\n",
    "\n",
    "imgnet_root='/workspace/data/data/imagenet_fall11/'\n",
    "imgnet_paths = load_from_seekhelper(imgnet_root)\n",
    "coco_inp_dir = '/workspace/data/data/coco/images/train2017'\n",
    "coco_paths = sorted(glob.glob(coco_inp_dir+'/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "590e95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_paths = [celeba_paths, coco_paths, imgnet_alt_paths, fashionpedia_paths, inat2019_paths, places365_paths, imgnet_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b54fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70000, 118287, 53542, 48823, 303593, 36500, 14197087]\n"
     ]
    }
   ],
   "source": [
    "print([len(p) for p in train_paths])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d0cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this step can be inserted for a second round to create a dedicated test set (all images not used during weight adaptation)\n",
    "#import numpy as np\n",
    "#precalc_dcts = np.load('ordered_dct_balanced.npz')\n",
    "#skip_paths = set([s.strip() for s in precalc_dcts['paths'].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25b81634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped  46520\n"
     ]
    }
   ],
   "source": [
    "#normalize sets and take smaller subsample (this is a random subset; use cell below instead to recreate the exact subsets from the paper)\n",
    "trg_num_samples_use = int(trg_num_samples*1.25) #about 12% of imagenet frames have one dimension smaller than 128 -> add double for buffer; will be straightened later\n",
    "all_paths_balanced = []\n",
    "skipped_cnt = 0\n",
    "for t in train_paths:\n",
    "    t0 = t[:]\n",
    "    random.shuffle(t0)\n",
    "    t1 = []\n",
    "    for t in t0:\n",
    "        if t.strip() in skip_paths:\n",
    "            skipped_cnt += 1\n",
    "            continue\n",
    "        t1.append(t)\n",
    "        if len(t1) >= trg_num_samples_use:\n",
    "            break\n",
    "    all_paths_balanced += t1\n",
    "print(\"Skipped \",skipped_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a6797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment to recreate the original data paths \n",
    "#!gunzip ordered_paths_balanced.txt.gz\n",
    "#with open('ordered_paths_balanced.txt', 'r') as f_out:\n",
    "#    all_paths_balanced = [p for p in f_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ba97263a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecaab41c7bc49e08a7e7c16320f084c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=17875.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from async_dct_loader import async_load_dct_paths, tqdm_nb\n",
    "from naphash_py import naphash as nhcpp, rot_inv_type\n",
    "nhcpp_objs = [nhcpp(dct_dim=dct_dim, rot_inv_mode=rot_inv_type.none, apply_center_crop=False, is_rgb=False) for _ in range(num_threads)] #no center crop\n",
    "dcts = await async_load_dct_paths(nhcpp_objs, all_paths_balanced, num_threads, dct_dim, min_img_dims, tqdm_vers=tqdm_nb, pil_sz=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e354db1",
   "metadata": {},
   "source": [
    "The next step balances the datasets (some files might not load or result in images smaller than 128x128)\n",
    "\n",
    "Each subset is represented trg_num_samples times (=16385)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "919a3ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 16385, 1: 16385, 2: 16385, 3: 16385, 4: 16385, 5: 16385, 6: 16385}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def dataset_by_path(p):\n",
    "    path_context = ['/celeba/','/coco/','/imagenet_alt/', '/fashionpedia/','/inat2019/','/places365/', '/imagenet_fall11/']\n",
    "    for i,c in enumerate(path_context):\n",
    "        if c in p: return i\n",
    "    return -1\n",
    "orig_dct, all_bu, count_ds, path_per_ds = [], [], {}, {i:[] for i in range(7)}\n",
    "all_paths_balanced = paths\n",
    "for i in range(len(all_paths_balanced)):\n",
    "    if dcts[i] is None:\n",
    "        continue\n",
    "    idx_dataset = dataset_by_path(all_paths_balanced[i])\n",
    "    if count_ds.get(idx_dataset,0) > trg_num_samples:\n",
    "        continue\n",
    "    path_per_ds[idx_dataset].append(all_paths_balanced[i])\n",
    "    count_ds[idx_dataset] = count_ds.get(idx_dataset,0) + 1  \n",
    "    all_bu.append(all_paths_balanced[i])\n",
    "print(count_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b0c78",
   "metadata": {},
   "source": [
    "We save the exact list used for experiments for later evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502271c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ordered_paths_balanced.txt', 'wt') as f_out:\n",
    "    for p in all_bu:\n",
    "        f_out.write(p.replace('\\n','')+'\\n')\n",
    "!gzip ordered_paths_balanced.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d123ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('ordered_dct_balanced.npz', dcts=orig_dct, paths=all_bu)  #round one: training set for weight adaptation\n",
    "#np.savez_compressed('ordered_dct_pil_testing.npz', dcts=orig_dct, paths=all_bu) #round two: test set for checking robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b362d",
   "metadata": {},
   "source": [
    "The steps below pre-calculate NPHASH hashes for the CIFAR10 dataset (this is done after weight adaptation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4172e6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f750e41c01c481aa98b198304186ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from async_dct_loader import async_load_dct_paths\n",
    "cifar10_train = list(sorted(files_in_subdirs('./cifar10/train')))\n",
    "cifar10_val = list(sorted(files_in_subdirs('./cifar10/test')))\n",
    "nhcpp_objs = [nhcpp(dct_dim=dct_dim, rot_inv_mode=rot_inv_type.none, apply_center_crop=False, is_rgb=False) for _ in range(num_threads)] #no center crop\n",
    "hashes = await async_load_dct_paths(nhcpp_objs, cifar10_val+cifar10_train, num_threads, dct_dim, -1, tqdm_vers=tqdm_nb, ret_hash = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cc4db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('cifar10_hashes.npz', hashes=np.vstack(hashes), paths=list(cifar10_val+cifar10_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
