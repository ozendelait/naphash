{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a11275",
   "metadata": {},
   "source": [
    "Preparing \"worst\" and \"best\" subsets per category in dataset bases on hash distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24f1436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cifar10_trainer import get_cat\n",
    "#see create_training_paths for creation of the precalculated dcts; \n",
    "#due to copyright these may present an issue when uploaded/clash with the open license -> you have to repeat the script yourself\n",
    "cifar10_hashes = np.load('cifar10_hashes.npz')\n",
    "hashes, paths = cifar10_hashes['hashes'], cifar10_hashes['paths']\n",
    "#calculate start/stop idx per category (should be in order val->train and per set in order of categories)\n",
    "idx = {}\n",
    "for i,p in enumerate(paths[10000:]):\n",
    "    idx.setdefault(get_cat(p),[]).append(i+10000)\n",
    "idx = {k:[min(v),max(v)+1] for k,v in idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f7b313",
   "metadata": {},
   "source": [
    "Helper functions to calculate hamming distance for numpy np.uint8 arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e42ea069",
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_counts = np.array([int(bin(x).count(\"1\")) for x in range(256)]).astype(np.uint8)\n",
    "def hamming_dist(a,b,axis=None):\n",
    "    return np.sum(bit_counts[np.bitwise_xor(a,b)],axis=axis)\n",
    "def hex2hash(s):\n",
    "    return np.frombuffer(bytes.fromhex(s), dtype=np.uint8)\n",
    "def hash2hex(h):\n",
    "    return h.tobytes().hex()\n",
    "def calc_inter_dist(hashes, eye_val=1024):\n",
    "    all_d = np.zeros((hashes.shape[0],hashes.shape[0]),dtype=np.int32)\n",
    "    for i in range(hashes.shape[0]):\n",
    "        all_d[i,i+1:] = hamming_dist(hashes[i],hashes[(i+1):], axis=1)\n",
    "    return all_d+all_d.T+np.eye(hashes.shape[0],dtype=np.int32)*eye_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328e704",
   "metadata": {},
   "source": [
    "This method calculates the \"best\" and \"worst\" subsets. See paper for description. \n",
    "\n",
    "The parameters allow additional subset heuristics to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "08060ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_list(dists_sqr, comb_vers=0):\n",
    "    max_idx = dists_sqr.shape[0]\n",
    "    if comb_vers < 2:\n",
    "        dists_fixed = dists_sqr*(1-np.eye(max_idx,max_idx,dtype=np.int32))\n",
    "    else:\n",
    "        max_val = np.max(dists_sqr)+1\n",
    "        dists_fixed = dists_sqr+np.eye(max_idx,max_idx,dtype=np.int32)*max_val\n",
    "    all_idx, fixed_idx = list(range(max_idx)), []\n",
    "    while len(fixed_idx) < max_idx:\n",
    "        if len(fixed_idx) == 0:\n",
    "            open_list = all_idx\n",
    "            check_list = dists_fixed\n",
    "        else:\n",
    "            open_list = list(set(all_idx).difference(set(fixed_idx)))\n",
    "            check_list = dists_fixed[fixed_idx][:,open_list]\n",
    "        if comb_vers == 0:\n",
    "            idx0 = np.argmin(np.max(check_list, axis = 0))\n",
    "        elif comb_vers == 1:\n",
    "            idx0 = np.argmax(np.max(check_list, axis = 0))\n",
    "        elif comb_vers == 2:\n",
    "            idx0 = np.argmin(np.min(check_list, axis = 0))\n",
    "        else:\n",
    "            idx0 = np.argmax(np.min(check_list, axis = 0))\n",
    "        idx1 = open_list[idx0]\n",
    "        fixed_idx.append(idx1)\n",
    "    return fixed_idx\n",
    "\n",
    "#apply \"best\"/\"worst\" heuristic individually per class\n",
    "def get_minimal_sets(hashes, idx, comb_vers = 0):\n",
    "    ret_sets = {}\n",
    "    for k,idx_m in idx.items():\n",
    "        dists32 = calc_inter_dist(hashes[idx_m[0]:idx_m[1]], 0)\n",
    "        dists_sqr = dists32*dists32\n",
    "        ret_sets[k] = [i+idx_m[0] for i in min_max_list(dists_sqr, comb_vers=comb_vers)]\n",
    "    return ret_sets\n",
    "\n",
    "# for comparision/reference calculate 10 random subsets\n",
    "def get_rand_seq(idx):\n",
    "    retseq = {}\n",
    "    for k, v in idx.items():\n",
    "        l0 = list(range(v[0],v[1]))\n",
    "        np.random.shuffle(l0)\n",
    "        retseq[k] = np.int32(l0).tolist()\n",
    "    return retseq\n",
    "\n",
    "\n",
    "worstset = get_minimal_sets(hashes, idx, comb_vers = 0)\n",
    "bestset = get_minimal_sets(hashes, idx, comb_vers = 3)\n",
    "randsets = {'r%i'%i:get_rand_seq(idx) for i in range(10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd33ec01",
   "metadata": {},
   "source": [
    "The subset paths are now stored in a json file (indices need conversions as json does not store np.int64 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "9db0a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "hashsets = {'worst': worstset, 'best': bestset}\n",
    "hash_sets_int ={k0:{k1:[int(i) for i in v1] for k1,v1 in v0.items()} for k0,v0 in hash_sets.items()}\n",
    "hash_sets_int['paths'] = paths.tolist()\n",
    "hash_sets_int.update(randsets)\n",
    "json.dump(hash_sets_int, open('cifar10_hashsets.json','wt'))\n",
    "!gzip cifar10_hashsets.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
